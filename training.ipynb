{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abgabe 3 - DQN zum autonomen Fahren\n",
    "\n",
    "Im Rahmen der dritten Abgabe im Kurs Künstliche Intelligenz erfolgte die Entwicklung und Implementierung eines Deep Q-Network (DQN) Agenten. \n",
    "Dieser soll optimale Entscheidungen in einer Autorennumgebung aus der gymnasium-Bibliothek, früher bekannt als gym, treffen.\n",
    "\n",
    "## 1 Import & Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "\n",
    "from agent.dqn import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als bereits fertige Bibliotheken werden einerseits *numpy* zur effizienten Berechnung von Matrizen und Vektoren, *cv2 (OpenCV)* zur Bildverarbeitung und *gymnasium* als Weiterentwicklung der gym-Bibliothek zur Simulation einer Autorennumgebung verwendet. Die Implementierung des DQN ist in eine separate Pythondatei ausgelagert. \n",
    "\n",
    "![Beispielbild](gym_environment.jpg)\n",
    "\n",
    "Die Reinforcement Learning Umgebung ist Teil der Box2D-Umgebungen der gymnasium-Bibliothek und simuliert ein Rennspiel aus der Vogelperspektive. Die generierte Rennstrecke ändert sich in jeder Episode zufällig. \n",
    "- Der Aktionsraum in der Umgebung kann kontinuierlich oder diskret sein. Im kontinuierlichen Fall gibt es drei Aktionen: Lenken (von -1 für volle Linkskurve bis +1 für volle Rechtskurve), Gas geben und Bremsen. In der Implementierung wird dieser in einen diskreten Aktionsraum mit 12 möglichen Aktionen umgewandelt. \n",
    "- Als Observationen werden RGB-Bilder (96x96 Pixel, 3 Kanäle) aus einer top-down Perspektive von dem Auto und der Rennstrecke generiert.\n",
    "- Die Belohnung beträgt -0,1 in jedem Frame und +1000/N für jede befahrene Tile der Rennstrecke, wobei N die Gesamtzahl der befahrenen Tiles auf der Rennstrecke ist. Zum Beispiel beträgt die Belohnung für einen Abschluss in 732 Frames 1000 - 0,1 * 732 = 926,8 Punkte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Das Training erfolgt auf einer GPU, um den Prozess zu beschleunigen. Das Training nahm insgesamt circa 7 Stunden in Anspruch und benötigte zwischenzeitlich 11 GB Arbeitsspeicher. Die Logik des Trainings sieht folgendermaßen aus:\n",
    "\n",
    "1. Zu Beginn wird eine Funktion process_state_image definiert, die das Eingangsbild verarbeitet. Es wird von Farb- zu Graustufen umgewandelt, um die Kanäle und damit die Anzahl der Eingabeparameter zu reduzieren.\n",
    "2. Die Variable action_space definiert eine Liste von Aktionen, die der Agent in der Umgebung ausführen kann. Jede Aktion besteht aus Werten für Lenken, Gasgeben und Bremsen.\n",
    "3. Die Variable state_shape gibt die Größe der Observation an, die ein 96x96 großes Graustufenbild ist.\n",
    "4. Die Umgebung \"CarRacing-v2\" wird mit gym.make initialisiert und anschließend der DQN-Agent erstellt.\n",
    "5. Der Agent wird in einer Schleife über n=100 bzw. später n=500 Episoden trainiert. In jeder Episode wird der Anfangszustand der Umgebung abgerufen. Der Agent interagiert dann mit der Umgebung, wählt Aktionen aus und sammelt Erfahrungen.\n",
    "6. Während der Interaktion mit der Umgebung werden Belohnungen gesammelt und negative Belohnungen werden gezählt, um sicherzustellen, dass der Agent das Auto nicht wahllos über Grass Tiles steuert.\n",
    "7. Der Agent speichert seine Erfahrungen in dem Replay-Buffer und führt alle 5 Frames ein Replay-Training durch, um das Modell zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 3.714814814814845\n",
      "Episode: 2, Total Reward: 1.7617363344051595\n",
      "Episode: 3, Total Reward: 1.844983818770244\n",
      "Episode: 4, Total Reward: 10.282733812949676\n",
      "Episode: 5, Total Reward: -0.09072164948451733\n",
      "Episode: 6, Total Reward: -0.09999999999998122\n",
      "Episode: 7, Total Reward: 5.964846416382285\n",
      "Episode: 8, Total Reward: -0.09090909090906973\n",
      "Episode: 9, Total Reward: -0.03157894736840314\n",
      "Episode: 10, Total Reward: -0.02727272727271865\n",
      "Episode: 11, Total Reward: 10.958823529411804\n",
      "Episode: 12, Total Reward: 32.22129963898913\n",
      "Episode: 13, Total Reward: -0.06603773584904293\n",
      "Episode: 14, Total Reward: 0.05241635687733853\n",
      "Episode: 15, Total Reward: 4.9771704180064695\n",
      "Episode: 16, Total Reward: -0.047311827956968616\n",
      "Episode: 17, Total Reward: 5.964846416382295\n",
      "Episode: 18, Total Reward: 7.487360594795579\n",
      "Episode: 19, Total Reward: 5.081229773462823\n",
      "Episode: 20, Total Reward: 14.922304832713701\n",
      "Episode: 21, Total Reward: 6.567844522968237\n",
      "Episode: 22, Total Reward: -0.0510948905109363\n",
      "Episode: 23, Total Reward: 5.906802721088473\n",
      "Episode: 24, Total Reward: 1.0951219512195385\n",
      "Episode: 25, Total Reward: 13.693388429752103\n",
      "Episode: 26, Total Reward: 3.5198830409357096\n",
      "Episode: 27, Total Reward: 9.952631578947392\n",
      "Episode: 28, Total Reward: -0.07619047619045927\n",
      "Episode: 29, Total Reward: 1.598412698412726\n",
      "Episode: 30, Total Reward: 2.0147540983606795\n",
      "Episode: 31, Total Reward: 18.505263157894664\n",
      "Episode: 32, Total Reward: 3.915015015015054\n",
      "Episode: 33, Total Reward: 30.167796610169383\n",
      "Episode: 34, Total Reward: 13.811032028469729\n",
      "Episode: 35, Total Reward: 12.311371237458232\n",
      "Episode: 36, Total Reward: 5.735016835016875\n",
      "Episode: 37, Total Reward: -0.061092150170627635\n",
      "Episode: 38, Total Reward: 2.9857142857142924\n",
      "Episode: 39, Total Reward: 7.01851851851856\n",
      "Episode: 40, Total Reward: 5.84915254237292\n",
      "Episode: 41, Total Reward: -0.08461538461536677\n",
      "Episode: 42, Total Reward: -0.05774647887323092\n",
      "Episode: 43, Total Reward: -0.0371647509578443\n",
      "Episode: 44, Total Reward: -0.0371647509578443\n",
      "Episode: 45, Total Reward: -0.05714285714284828\n",
      "Episode: 46, Total Reward: -0.09126213592231225\n",
      "Episode: 47, Total Reward: -0.06603773584904649\n",
      "Episode: 48, Total Reward: 2.50544217687078\n",
      "Episode: 49, Total Reward: 5.622408026755892\n",
      "Episode: 50, Total Reward: 36.51904761904754\n",
      "Episode: 51, Total Reward: 4.773015873015911\n",
      "Episode: 52, Total Reward: 4.9771704180064695\n",
      "Episode: 53, Total Reward: 14.878073089701036\n",
      "Episode: 54, Total Reward: 1.9293159609120702\n",
      "Episode: 55, Total Reward: 5.791891891891932\n",
      "Episode: 56, Total Reward: 6.201038062283777\n",
      "Episode: 57, Total Reward: -0.05686274509802858\n",
      "Episode: 58, Total Reward: 7.3501845018450584\n",
      "Episode: 59, Total Reward: 7.4185185185185585\n",
      "Episode: 60, Total Reward: 5.2934426229508595\n",
      "Episode: 61, Total Reward: 7.487360594795579\n",
      "Episode: 62, Total Reward: 11.976923076923118\n",
      "Episode: 63, Total Reward: 5.1562913907285095\n",
      "Episode: 64, Total Reward: 6.885611510791406\n",
      "Episode: 65, Total Reward: 6.082130584192478\n",
      "Episode: 66, Total Reward: -0.0034482758620619502\n",
      "Episode: 67, Total Reward: 2.5518771331058177\n",
      "Episode: 68, Total Reward: -0.006993006992998846\n",
      "Episode: 69, Total Reward: -0.05974025974024255\n",
      "Episode: 70, Total Reward: 0.07318435754189795\n",
      "Episode: 71, Total Reward: 3.1857142857143095\n",
      "Episode: 72, Total Reward: -0.04345403899721287\n",
      "Episode: 73, Total Reward: 6.450184501845061\n",
      "Episode: 74, Total Reward: 0.011111111111133415\n",
      "Episode: 75, Total Reward: 3.8253731343283874\n",
      "Episode: 76, Total Reward: 7.3501845018450584\n",
      "Episode: 77, Total Reward: 3.572870662460589\n",
      "Episode: 78, Total Reward: 2.459322033898318\n",
      "Episode: 79, Total Reward: 2.551877133105823\n",
      "Episode: 80, Total Reward: 5.821602787456483\n",
      "Episode: 81, Total Reward: 2.9350877192982767\n",
      "Episode: 82, Total Reward: 8.46115702479343\n",
      "Episode: 83, Total Reward: 9.005923344947776\n",
      "Episode: 84, Total Reward: 6.141379310344867\n",
      "Episode: 85, Total Reward: 3.5627565982404925\n",
      "Episode: 86, Total Reward: 7.3501845018450584\n",
      "Episode: 87, Total Reward: 8.58503937007878\n",
      "Episode: 88, Total Reward: 8.66284584980241\n",
      "Episode: 89, Total Reward: 4.773015873015909\n",
      "Episode: 90, Total Reward: 3.2798761609907228\n",
      "Episode: 91, Total Reward: 1.9718954248366143\n",
      "Episode: 92, Total Reward: 5.4491525423729135\n",
      "Episode: 93, Total Reward: 4.522408026755876\n",
      "Episode: 94, Total Reward: 1.8032258064516178\n",
      "Episode: 95, Total Reward: 5.664846416382283\n",
      "Episode: 96, Total Reward: 7.182352941176509\n",
      "Episode: 97, Total Reward: 6.385611510791405\n",
      "Episode: 98, Total Reward: 5.630496453900738\n",
      "Episode: 99, Total Reward: 5.391891891891925\n",
      "Episode: 100, Total Reward: 1.1324159021406994\n"
     ]
    }
   ],
   "source": [
    "def process_state_image(state):\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = state.astype(float)\n",
    "    return state / 255.0\n",
    "\n",
    "action_space = [\n",
    "    # (wheel [-1~1], gas [0~1], break [0~1])\n",
    "    (-1, 1, 0.5), (0, 1, 0.5), (1, 1, 0.5),\n",
    "    (-1, 1,   0), (0, 1,   0), (1, 1,   0),\n",
    "    (-1, 0, 0.5), (0, 0, 0.5), (1, 0, 0.5), \n",
    "    (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
    "]\n",
    "# 96x96 grayscaled image\n",
    "state_shape = (96, 96, 1)\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "agent = DQN(action_space, state_shape)\n",
    "\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    # Reduce rgb image to gray scale\n",
    "    state = process_state_image(state)\n",
    "    total_reward = 0\n",
    "    negative_reward = 0\n",
    "    frame_counter = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Reduce rgb image to gray scale\n",
    "        next_state = process_state_image(next_state)\n",
    "\n",
    "        total_reward += reward\n",
    "        frame_counter += 1\n",
    "\n",
    "        if frame_counter > 100 and reward < 0:\n",
    "            # Don't get lost on grass\n",
    "            negative_reward += 1\n",
    "        else:\n",
    "            # Reset if agent finds track\n",
    "            negative_reward = 0\n",
    "\n",
    "        if negative_reward > 10 or total_reward < 0:\n",
    "            done = True # End early\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        if frame_counter % 5 == 0:\n",
    "            agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    if episode > 0 and episode % 5 == 0:\n",
    "        # Save model every 100th episode\n",
    "        agent.model.save_weights(f\"models/trial_{episode}/dqn\")\n",
    "    \n",
    "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
    "    reward_history += [total_reward]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00998645168764533 0.01\n"
     ]
    }
   ],
   "source": [
    "print(agent.epsilon, agent.epsilon_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem ersten Training über 100 Episoden kann der durchschnittliche Reward von 2 auf circa 5 Punkte leicht verbessert werden. Das Training dazu hat über GPU 90 Minuten gedauert. Zudem sinkt der Epsilonwert durch die kontinuierliche Verringerung unter das definierte Minimum von 0.01, sodass die Exploration voll ausgeschöpft wurde und nur noch 1% der Aktionen zufällig gewählt werden. Um weitere Forschritte zu erzielen, wird das Training fortgeführt und nun 400 weitere Episoden trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 101, Total Reward: 5.081229773462823\n",
      "Episode: 102, Total Reward: 8.90000000000004\n",
      "Episode: 103, Total Reward: 6.261111111111152\n",
      "Episode: 104, Total Reward: 5.511295681063161\n",
      "Episode: 105, Total Reward: 6.885611510791405\n",
      "Episode: 106, Total Reward: 4.925641025641066\n",
      "Episode: 107, Total Reward: 5.906802721088473\n",
      "Episode: 108, Total Reward: 7.767924528301927\n",
      "Episode: 109, Total Reward: 4.773015873015911\n",
      "Episode: 110, Total Reward: 4.3798761609907455\n",
      "Episode: 111, Total Reward: 7.3501845018450584\n",
      "Episode: 112, Total Reward: 7.148175182481792\n",
      "Episode: 113, Total Reward: 6.757142857142897\n",
      "Episode: 114, Total Reward: 5.622408026755892\n",
      "Episode: 115, Total Reward: 6.082130584192479\n",
      "Episode: 116, Total Reward: 7.015942028985547\n",
      "Episode: 117, Total Reward: 5.791891891891932\n",
      "Episode: 118, Total Reward: 5.186644951140105\n",
      "Episode: 119, Total Reward: 5.491891891891926\n",
      "Episode: 120, Total Reward: 4.403875968992287\n",
      "Episode: 121, Total Reward: -0.006329113924033097\n",
      "Episode: 122, Total Reward: 7.6265917602996645\n",
      "Episode: 123, Total Reward: 5.081229773462823\n",
      "Episode: 124, Total Reward: 5.266666666666699\n",
      "Episode: 125, Total Reward: -0.06505576208177488\n",
      "Episode: 126, Total Reward: -0.05555555555554759\n",
      "Episode: 127, Total Reward: -0.010223642172517894\n",
      "Episode: 128, Total Reward: -0.09933993399339269\n",
      "Episode: 129, Total Reward: -0.05774647887323092\n",
      "Episode: 130, Total Reward: -0.09933993399339269\n",
      "Episode: 131, Total Reward: -0.07406143344709126\n",
      "Episode: 132, Total Reward: -0.006993006992998846\n",
      "Episode: 133, Total Reward: 7.418518518518557\n",
      "Episode: 134, Total Reward: 2.3680134680135008\n",
      "Episode: 135, Total Reward: 8.741269841269881\n",
      "Episode: 136, Total Reward: -0.036619718309839316\n",
      "Episode: 137, Total Reward: 44.952427184465975\n",
      "Episode: 138, Total Reward: 51.662962962962894\n",
      "Episode: 139, Total Reward: 70.20081300813018\n",
      "Episode: 140, Total Reward: 7.248623853211049\n",
      "Episode: 141, Total Reward: 5.735016835016868\n",
      "Episode: 142, Total Reward: 6.950541516245526\n",
      "Episode: 143, Total Reward: 5.456291390728516\n",
      "Episode: 144, Total Reward: 24.566666666666656\n",
      "Episode: 145, Total Reward: 15.019402985074642\n",
      "Episode: 146, Total Reward: -0.08749999999998903\n",
      "Episode: 147, Total Reward: 5.366666666666699\n",
      "Episode: 148, Total Reward: 8.966889632107062\n",
      "Episode: 149, Total Reward: -0.011036789297652144\n",
      "Episode: 150, Total Reward: -0.05548172757474412\n",
      "Episode: 151, Total Reward: -0.04705882352940291\n",
      "Episode: 152, Total Reward: -0.006993006992998846\n",
      "Episode: 153, Total Reward: -0.05420560747661907\n",
      "Episode: 154, Total Reward: 25.323841059602593\n",
      "Episode: 155, Total Reward: 6.382517482517523\n",
      "Episode: 156, Total Reward: 79.90521172638448\n",
      "Episode: 157, Total Reward: 64.8075907590759\n",
      "Episode: 158, Total Reward: 120.53513513513528\n",
      "Episode: 159, Total Reward: 1.5582278481012728\n",
      "Episode: 160, Total Reward: 14.262318840579692\n",
      "Episode: 161, Total Reward: 19.93448275862064\n",
      "Episode: 162, Total Reward: 51.3891891891891\n",
      "Episode: 163, Total Reward: 9.589655172413822\n",
      "Episode: 164, Total Reward: 7.767924528301927\n",
      "Episode: 165, Total Reward: -0.04703832752612744\n",
      "Episode: 166, Total Reward: 2.6931034482758927\n",
      "Episode: 167, Total Reward: -0.05548172757474412\n",
      "Episode: 168, Total Reward: -0.07088607594936147\n",
      "Episode: 169, Total Reward: 8.254838709677456\n",
      "Episode: 170, Total Reward: 5.622408026755892\n",
      "Episode: 171, Total Reward: -0.032862190812713105\n",
      "Episode: 172, Total Reward: -0.032862190812713105\n",
      "Episode: 173, Total Reward: 2.645704467353955\n",
      "Episode: 174, Total Reward: 4.051515151515185\n",
      "Episode: 175, Total Reward: 2.5986301369863143\n",
      "Episode: 176, Total Reward: 7.767924528301924\n",
      "Episode: 177, Total Reward: 4.623270440251613\n",
      "Episode: 178, Total Reward: 5.84915254237292\n",
      "Episode: 179, Total Reward: 1.021212121212134\n",
      "Episode: 180, Total Reward: 4.025000000000025\n",
      "Episode: 181, Total Reward: 5.950541516245513\n",
      "Episode: 182, Total Reward: 5.029032258064557\n",
      "Episode: 183, Total Reward: 7.148175182481792\n",
      "Episode: 184, Total Reward: -0.06923076923075233\n",
      "Episode: 185, Total Reward: -0.08245614035086946\n",
      "Episode: 186, Total Reward: -0.0659932659932585\n",
      "Episode: 187, Total Reward: 4.925641025641067\n",
      "Episode: 188, Total Reward: 5.622408026755892\n",
      "Episode: 189, Total Reward: 11.627272727272768\n",
      "Episode: 190, Total Reward: 11.127272727272768\n",
      "Episode: 191, Total Reward: -0.019653179190747688\n",
      "Episode: 192, Total Reward: -0.0885906040268385\n",
      "Episode: 193, Total Reward: -0.005755395683445047\n",
      "Episode: 194, Total Reward: -0.031541218637984175\n",
      "Episode: 195, Total Reward: 3.1348754448398757\n",
      "Episode: 196, Total Reward: 6.693594306049862\n",
      "Episode: 197, Total Reward: 4.37987616099075\n",
      "Episode: 198, Total Reward: 8.355252918287977\n",
      "Episode: 199, Total Reward: 5.3473684210526695\n",
      "Episode: 200, Total Reward: 6.201038062283777\n",
      "Episode: 201, Total Reward: 9.539130434782653\n",
      "Episode: 202, Total Reward: 7.282352941176509\n",
      "Episode: 203, Total Reward: 8.980321285140603\n",
      "Episode: 204, Total Reward: 5.964846416382294\n",
      "Episode: 205, Total Reward: 8.38051948051952\n",
      "Episode: 206, Total Reward: -0.06967509025268659\n",
      "Episode: 207, Total Reward: -0.09607843137253202\n",
      "Episode: 208, Total Reward: 4.167175572519124\n",
      "Episode: 209, Total Reward: 4.1905198776758805\n",
      "Episode: 210, Total Reward: -0.02602739726025513\n",
      "Episode: 211, Total Reward: 5.401650165016541\n",
      "Episode: 212, Total Reward: 10.70685358255455\n",
      "Episode: 213, Total Reward: 2.6931034482758944\n",
      "Episode: 214, Total Reward: 3.4772594752186956\n",
      "Episode: 215, Total Reward: 6.821146953405057\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "for episode in range(100, num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = process_state_image(state)\n",
    "    total_reward = 0\n",
    "    negative_reward = 0\n",
    "    frame_counter = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        next_state = process_state_image(next_state)\n",
    "\n",
    "        total_reward += reward\n",
    "        frame_counter += 1\n",
    "\n",
    "        if frame_counter > 100 and reward < 0:\n",
    "            negative_reward += 1\n",
    "        else:\n",
    "            negative_reward = 0\n",
    "\n",
    "        if negative_reward > 10 or total_reward < 0:\n",
    "            done = True # End early\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        if frame_counter % 5 == 0:\n",
    "            agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    if episode > 0 and episode % 20 == 0:\n",
    "        # Save every 100th episode\n",
    "        agent.model.save_weights(f\"models/trial_{episode}/dqn\")\n",
    "    \n",
    "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
    "    reward_history += [total_reward]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach 215 Episoden kam es leider zu einem Out-Of-Memory-Fehler, sodass das Training ab Episode 200 wieder aufgenommen wurde. Der Replaybuffer kann jedoch nicht wiederhergestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 201, Total Reward: 6.505633802816941\n",
      "Episode: 202, Total Reward: -0.09072164948453376\n",
      "Episode: 203, Total Reward: 2.2333333333333654\n",
      "Episode: 204, Total Reward: 3.9091254752851996\n",
      "Episode: 205, Total Reward: 1.6795527156549763\n",
      "Episode: 206, Total Reward: 11.456390977443649\n",
      "Episode: 207, Total Reward: -0.029889298892965516\n",
      "Episode: 208, Total Reward: 9.238983050847496\n",
      "Episode: 209, Total Reward: 5.94545454545458\n",
      "Episode: 210, Total Reward: 9.91818181818186\n",
      "Episode: 211, Total Reward: 5.133766233766272\n",
      "Episode: 212, Total Reward: 0.8047619047619305\n",
      "Episode: 213, Total Reward: 5.511295681063162\n",
      "Episode: 214, Total Reward: 3.5250000000000266\n",
      "Episode: 215, Total Reward: 18.097080291970766\n",
      "Episode: 216, Total Reward: -0.01098901098898894\n",
      "Episode: 217, Total Reward: -0.01098901098898894\n",
      "Episode: 218, Total Reward: 4.9771704180064695\n",
      "Episode: 219, Total Reward: 9.805923344947773\n",
      "Episode: 220, Total Reward: 6.50563380281694\n",
      "Episode: 221, Total Reward: 8.069329073482468\n",
      "Episode: 222, Total Reward: 10.101413427561877\n",
      "Episode: 223, Total Reward: 6.505633802816941\n",
      "Episode: 224, Total Reward: 14.323728813559308\n",
      "Episode: 225, Total Reward: 38.02280701754377\n",
      "Episode: 226, Total Reward: -0.006993006992998846\n",
      "Episode: 227, Total Reward: 32.97179487179484\n",
      "Episode: 228, Total Reward: 56.415658362989234\n",
      "Episode: 229, Total Reward: 6.630496453900726\n",
      "Episode: 230, Total Reward: -0.05517241379308513\n",
      "Episode: 231, Total Reward: 52.873063973063914\n",
      "Episode: 232, Total Reward: 102.09403973509957\n",
      "Episode: 233, Total Reward: 1.4786163522012865\n",
      "Episode: 234, Total Reward: 9.447945205479488\n",
      "Episode: 235, Total Reward: -0.0659932659932585\n",
      "Episode: 236, Total Reward: 83.27553516819609\n",
      "Episode: 237, Total Reward: 57.628522336769684\n",
      "Episode: 238, Total Reward: 39.37318611987374\n",
      "Episode: 239, Total Reward: 54.45714285714279\n",
      "Episode: 240, Total Reward: 46.475757575757505\n",
      "Episode: 241, Total Reward: 59.13411371237449\n",
      "Episode: 242, Total Reward: 48.00868167202561\n",
      "Episode: 243, Total Reward: 54.117391304347734\n",
      "Episode: 244, Total Reward: 121.81386138613904\n",
      "Episode: 245, Total Reward: 121.10588235294135\n",
      "Episode: 246, Total Reward: 101.23157894736859\n",
      "Episode: 247, Total Reward: 64.16881720430113\n",
      "Episode: 248, Total Reward: 47.92777777777768\n",
      "Episode: 249, Total Reward: 112.73698630137018\n",
      "Episode: 250, Total Reward: 41.79127516778516\n",
      "Episode: 251, Total Reward: 5.081229773462823\n",
      "Episode: 252, Total Reward: 14.079856115107926\n",
      "Episode: 253, Total Reward: 58.83006993006985\n",
      "Episode: 254, Total Reward: 69.35977011494263\n",
      "Episode: 255, Total Reward: 53.85947712418293\n",
      "Episode: 256, Total Reward: 50.882389937106915\n",
      "Episode: 257, Total Reward: 49.522923588039745\n",
      "Episode: 258, Total Reward: 51.1999999999999\n",
      "Episode: 259, Total Reward: 8.572131147541024\n",
      "Episode: 260, Total Reward: 62.32657342657334\n",
      "Episode: 261, Total Reward: 52.92018348623846\n",
      "Episode: 262, Total Reward: 49.21746031746026\n",
      "Episode: 263, Total Reward: 50.8524590163934\n",
      "Episode: 264, Total Reward: 4.87444089456873\n",
      "Episode: 265, Total Reward: 8.205019305019343\n",
      "Episode: 266, Total Reward: 5.239869281045792\n",
      "Episode: 267, Total Reward: 49.736501901140606\n",
      "Episode: 268, Total Reward: 52.8666666666666\n",
      "Episode: 269, Total Reward: 53.83506493506483\n",
      "Episode: 270, Total Reward: 19.788030888030857\n",
      "Episode: 271, Total Reward: 9.858083832335366\n",
      "Episode: 272, Total Reward: 44.455555555555485\n",
      "Episode: 273, Total Reward: 86.46731517509743\n",
      "Episode: 274, Total Reward: 9.805923344947773\n",
      "Episode: 275, Total Reward: 70.63333333333338\n",
      "Episode: 276, Total Reward: 60.32857142857135\n",
      "Episode: 277, Total Reward: 6.261111111111152\n",
      "Episode: 278, Total Reward: -0.029889298892965516\n",
      "Episode: 279, Total Reward: 57.84444444444437\n",
      "Episode: 280, Total Reward: 62.74366197183093\n",
      "Episode: 281, Total Reward: 2.1890365448505293\n",
      "Episode: 282, Total Reward: -0.07399267399266507\n",
      "Episode: 283, Total Reward: 56.6419354838709\n",
      "Episode: 284, Total Reward: 61.384210526315684\n",
      "Episode: 285, Total Reward: 52.65245901639342\n",
      "Episode: 286, Total Reward: 105.98111888111906\n",
      "Episode: 287, Total Reward: 60.58458781361997\n",
      "Episode: 288, Total Reward: 51.2145400593471\n",
      "Episode: 289, Total Reward: 35.20842911877393\n",
      "Episode: 290, Total Reward: 54.84532374100715\n",
      "Episode: 291, Total Reward: 56.20769230769222\n",
      "Episode: 292, Total Reward: 95.10485436893238\n",
      "Episode: 293, Total Reward: 50.224999999999945\n",
      "Episode: 294, Total Reward: 56.56435986159163\n",
      "Episode: 295, Total Reward: 61.627272727272604\n",
      "Episode: 296, Total Reward: 12.709523809523832\n",
      "Episode: 297, Total Reward: 51.86296296296288\n",
      "Episode: 298, Total Reward: 2.3518771331058166\n",
      "Episode: 299, Total Reward: 52.233333333333285\n",
      "Episode: 300, Total Reward: 154.8731343283585\n",
      "Episode: 301, Total Reward: 100.67164179104506\n",
      "Episode: 302, Total Reward: 34.80163934426225\n",
      "Episode: 303, Total Reward: 96.85949367088618\n",
      "Episode: 304, Total Reward: 8.833554817275786\n",
      "Episode: 305, Total Reward: 12.622695035461033\n",
      "Episode: 306, Total Reward: 50.757894736842\n",
      "Episode: 307, Total Reward: 26.44266211604086\n",
      "Episode: 308, Total Reward: 34.52737642585542\n",
      "Episode: 309, Total Reward: 26.57123287671224\n",
      "Episode: 310, Total Reward: 81.79459459459467\n",
      "Episode: 311, Total Reward: 45.37840531561454\n",
      "Episode: 312, Total Reward: 145.5335766423361\n",
      "Episode: 313, Total Reward: 41.23018867924519\n",
      "Episode: 314, Total Reward: 23.743205574912793\n",
      "Episode: 315, Total Reward: 31.4363957597173\n",
      "Episode: 316, Total Reward: 68.12530120481932\n",
      "Episode: 317, Total Reward: 5.111295681063157\n",
      "Episode: 318, Total Reward: 5.1337662337662735\n",
      "Episode: 319, Total Reward: 56.31573033707857\n",
      "Episode: 320, Total Reward: 10.797810218978109\n",
      "Episode: 321, Total Reward: 4.823566878980932\n",
      "Episode: 322, Total Reward: 13.547887323943614\n",
      "Episode: 323, Total Reward: 8.50784313725494\n",
      "Episode: 324, Total Reward: 73.11052631578956\n",
      "Episode: 325, Total Reward: 22.889297658862887\n",
      "Episode: 326, Total Reward: 50.333447098976045\n",
      "Episode: 327, Total Reward: 10.32857142857147\n",
      "Episode: 328, Total Reward: 24.194117647058736\n",
      "Episode: 329, Total Reward: 61.143346007604464\n",
      "Episode: 330, Total Reward: 2.6457044673539842\n",
      "Episode: 331, Total Reward: 32.40379746835437\n",
      "Episode: 332, Total Reward: 35.87986577181201\n",
      "Episode: 333, Total Reward: 75.52260536398474\n",
      "Episode: 334, Total Reward: 8.833554817275786\n",
      "Episode: 335, Total Reward: 80.65000000000013\n",
      "Episode: 336, Total Reward: 8.900000000000041\n",
      "Episode: 337, Total Reward: 24.23568904593635\n",
      "Episode: 338, Total Reward: 35.87986577181203\n",
      "Episode: 339, Total Reward: 11.415384615384642\n",
      "Episode: 340, Total Reward: 21.158064516128945\n",
      "Episode: 341, Total Reward: 80.67042253521139\n",
      "Episode: 342, Total Reward: 68.86666666666675\n",
      "Episode: 343, Total Reward: 19.101342281879173\n",
      "Episode: 344, Total Reward: 38.58944099378877\n",
      "Episode: 345, Total Reward: 50.54362139917694\n",
      "Episode: 346, Total Reward: 58.84244604316537\n",
      "Episode: 347, Total Reward: 21.638095238095175\n",
      "Episode: 348, Total Reward: 11.040221402214023\n",
      "Episode: 349, Total Reward: 25.530036630036573\n",
      "Episode: 350, Total Reward: 39.435114503816756\n",
      "Episode: 351, Total Reward: 27.094444444444406\n",
      "Episode: 352, Total Reward: 13.423472668810332\n",
      "Episode: 353, Total Reward: 53.41612903225797\n",
      "Episode: 354, Total Reward: 27.09444444444441\n",
      "Episode: 355, Total Reward: 13.591358024691377\n",
      "Episode: 356, Total Reward: 25.144850498338858\n",
      "Episode: 357, Total Reward: 22.12259136212619\n",
      "Episode: 358, Total Reward: 23.50207612456739\n",
      "Episode: 359, Total Reward: 7.148175182481792\n",
      "Episode: 360, Total Reward: 40.67993527508082\n",
      "Episode: 361, Total Reward: 84.84095940959423\n",
      "Episode: 362, Total Reward: 72.80555555555564\n",
      "Episode: 363, Total Reward: 6.567844522968237\n",
      "Episode: 364, Total Reward: 21.15806451612899\n",
      "Episode: 365, Total Reward: 123.10506329113947\n",
      "Episode: 366, Total Reward: 132.19446254071698\n",
      "Episode: 367, Total Reward: 25.203630363036226\n",
      "Episode: 368, Total Reward: 48.82509363295872\n",
      "Episode: 369, Total Reward: 81.04095940959422\n",
      "Episode: 370, Total Reward: 34.5026058631921\n",
      "Episode: 371, Total Reward: 6.141379310344865\n",
      "Episode: 372, Total Reward: 16.58166089965393\n",
      "Episode: 373, Total Reward: 11.122222222222225\n",
      "Episode: 374, Total Reward: 62.25894039735088\n",
      "Episode: 375, Total Reward: 13.205555555555538\n",
      "Episode: 376, Total Reward: 63.10494699646639\n",
      "Episode: 377, Total Reward: 98.91111111111124\n",
      "Episode: 378, Total Reward: 19.788030888030878\n",
      "Episode: 379, Total Reward: 12.078807947019868\n",
      "Episode: 380, Total Reward: 14.07985611510787\n",
      "Episode: 381, Total Reward: 80.24929577464799\n",
      "Episode: 382, Total Reward: 8.205019305019343\n",
      "Episode: 383, Total Reward: 51.64509803921562\n",
      "Episode: 384, Total Reward: 0.9481927710843637\n",
      "Episode: 385, Total Reward: 33.017647058823435\n",
      "Episode: 386, Total Reward: 67.26712328767127\n",
      "Episode: 387, Total Reward: 76.00000000000009\n"
     ]
    }
   ],
   "source": [
    "def process_state_image(state):\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = state.astype(float)\n",
    "    return state / 255.0\n",
    "\n",
    "action_space = [\n",
    "    (-1, 1, 0.5), (0, 1, 0.5), (1, 1, 0.5), # Action Space Structure\n",
    "    (-1, 1,   0), (0, 1,   0), (1, 1,   0), # (Steering Wheel, Gas, Break)\n",
    "    (-1, 0, 0.5), (0, 0, 0.5), (1, 0, 0.5), # Range -1~1       0~1   0~1\n",
    "    (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
    "]\n",
    "state_shape = (96, 96, 1)\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "agent = DQN(action_space, state_shape)\n",
    "agent.model.load_weights(\"models/trial_200o/dqn\")\n",
    "agent.epsilon = agent.epsilon_min\n",
    "\n",
    "num_episodes = 500\n",
    "for episode in range(200, num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = process_state_image(state)\n",
    "    total_reward = 0\n",
    "    negative_reward = 0\n",
    "    frame_counter = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        next_state = process_state_image(next_state)\n",
    "\n",
    "        total_reward += reward\n",
    "        frame_counter += 1\n",
    "\n",
    "        if frame_counter > 100 and reward < 0:\n",
    "            negative_reward += 1\n",
    "        else:\n",
    "            negative_reward = 0\n",
    "\n",
    "        if negative_reward > 10 or total_reward < 0:\n",
    "            done = True # End early\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        if frame_counter % 5 == 0:\n",
    "            agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    if episode > 0 and episode % 5 == 0:\n",
    "        # Save model every 5th episode\n",
    "        agent.model.save_weights(f\"models/trial_{episode}/dqn\")\n",
    "    \n",
    "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
    "    reward_history += [total_reward]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach 387 Episoden kam es erneut zu einem Out-Of-Memory-Fehler, sodass ab Episode 385 das Training wieder aufgenommen werden konnte. Zugleich zeichnet sich ab circa Episode 210 ein Anstieg im Reward ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 386, Total Reward: 74.21329305135961\n",
      "Episode: 387, Total Reward: 148.42252559727\n",
      "Episode: 388, Total Reward: 53.41612903225799\n",
      "Episode: 389, Total Reward: 78.52258064516137\n",
      "Episode: 390, Total Reward: 85.62222222222238\n",
      "Episode: 391, Total Reward: 56.5886685552407\n",
      "Episode: 392, Total Reward: 23.98771929824555\n",
      "Episode: 393, Total Reward: 17.838906752411496\n",
      "Episode: 394, Total Reward: 18.42029520295194\n",
      "Episode: 395, Total Reward: 67.47142857142858\n",
      "Episode: 396, Total Reward: 8.767549668874212\n",
      "Episode: 397, Total Reward: 86.837748344371\n",
      "Episode: 398, Total Reward: 14.459105431309851\n",
      "Episode: 399, Total Reward: 5.566666666666707\n",
      "Episode: 400, Total Reward: 56.2758865248226\n",
      "Episode: 401, Total Reward: 80.15475285171111\n",
      "Episode: 402, Total Reward: 23.389297658862848\n",
      "Episode: 403, Total Reward: 65.82307692307695\n",
      "Episode: 404, Total Reward: 89.8411764705884\n",
      "Episode: 405, Total Reward: 10.718181818181845\n",
      "Episode: 406, Total Reward: 18.57359050445095\n",
      "Episode: 407, Total Reward: 9.102020202020233\n",
      "Episode: 408, Total Reward: 75.64788273615646\n",
      "Episode: 409, Total Reward: 54.19209621993119\n",
      "Episode: 410, Total Reward: 6.443859649122846\n",
      "Episode: 411, Total Reward: 42.77205387205384\n",
      "Episode: 412, Total Reward: 6.082130584192479\n",
      "Episode: 413, Total Reward: 104.28823529411783\n",
      "Episode: 414, Total Reward: 52.57041198501866\n",
      "Episode: 415, Total Reward: 12.429411764705886\n",
      "Episode: 416, Total Reward: 20.62846975088969\n",
      "Episode: 417, Total Reward: 67.92820512820516\n",
      "Episode: 418, Total Reward: 11.854982817869459\n",
      "Episode: 419, Total Reward: 70.83333333333337\n",
      "Episode: 420, Total Reward: 88.85853658536594\n",
      "Episode: 421, Total Reward: 17.930030030030046\n",
      "Episode: 422, Total Reward: 18.39852507374625\n",
      "Episode: 423, Total Reward: 76.23529411764719\n",
      "Episode: 424, Total Reward: 47.99209621993124\n",
      "Episode: 425, Total Reward: 17.93225806451605\n",
      "Episode: 426, Total Reward: 5.456291390728516\n",
      "Episode: 427, Total Reward: 20.545569620253122\n",
      "Episode: 428, Total Reward: 12.85209580838324\n",
      "Episode: 429, Total Reward: 8.636842105263195\n",
      "Episode: 430, Total Reward: 45.566666666666606\n",
      "Episode: 431, Total Reward: 3.44545454545458\n",
      "Episode: 432, Total Reward: 55.44006734006729\n",
      "Episode: 433, Total Reward: 73.53093525179861\n",
      "Episode: 434, Total Reward: 65.22307692307695\n",
      "Episode: 435, Total Reward: 7.215018315018353\n",
      "Episode: 436, Total Reward: 5.964846416382288\n",
      "Episode: 437, Total Reward: 39.624637681159356\n",
      "Episode: 438, Total Reward: 18.70132450331119\n",
      "Episode: 439, Total Reward: 94.46643598615931\n",
      "Episode: 440, Total Reward: 6.950541516245526\n",
      "Episode: 441, Total Reward: 6.443859649122846\n",
      "Episode: 442, Total Reward: 9.377815699658742\n",
      "Episode: 443, Total Reward: 24.899999999999913\n",
      "Episode: 444, Total Reward: 70.29934640522885\n",
      "Episode: 445, Total Reward: 6.693594306049862\n",
      "Episode: 446, Total Reward: 79.7566878980893\n",
      "Episode: 447, Total Reward: 20.14999999999992\n",
      "Episode: 448, Total Reward: 42.66344086021496\n",
      "Episode: 449, Total Reward: 24.487188612099562\n",
      "Episode: 450, Total Reward: 45.43710247349815\n",
      "Episode: 451, Total Reward: 19.31825095057029\n",
      "Episode: 452, Total Reward: 11.127272727272768\n",
      "Episode: 453, Total Reward: 17.838906752411496\n",
      "Episode: 454, Total Reward: 47.91639344262289\n",
      "Episode: 455, Total Reward: 8.38051948051952\n",
      "Episode: 456, Total Reward: 80.02807017543869\n",
      "Episode: 457, Total Reward: 64.43956834532378\n",
      "Episode: 458, Total Reward: 20.520553359683735\n",
      "Episode: 459, Total Reward: 11.888505747126423\n",
      "Episode: 460, Total Reward: 19.551340996168552\n",
      "Episode: 461, Total Reward: 88.3944444444446\n",
      "Episode: 462, Total Reward: 53.293939393939326\n",
      "Episode: 463, Total Reward: 102.02450592885386\n",
      "Episode: 464, Total Reward: 42.7130990415335\n",
      "Episode: 465, Total Reward: 10.241463414634186\n",
      "Episode: 466, Total Reward: 71.25294117647069\n",
      "Episode: 467, Total Reward: 17.676978417266145\n",
      "Episode: 468, Total Reward: 32.90951008645522\n",
      "Episode: 469, Total Reward: 83.96313993174076\n",
      "Episode: 470, Total Reward: 17.024999999999924\n",
      "Episode: 471, Total Reward: 46.47101449275353\n",
      "Episode: 472, Total Reward: 102.33278688524612\n",
      "Episode: 473, Total Reward: 50.68656716417907\n",
      "Episode: 474, Total Reward: 14.789967637540402\n",
      "Episode: 475, Total Reward: 8.31747572815538\n",
      "Episode: 476, Total Reward: 76.7615384615385\n",
      "Episode: 477, Total Reward: 56.24006734006729\n",
      "Episode: 478, Total Reward: 7.947619047619085\n",
      "Episode: 479, Total Reward: 83.39629629629643\n",
      "Episode: 480, Total Reward: 80.48358208955233\n",
      "Episode: 481, Total Reward: 40.41515151515144\n",
      "Episode: 482, Total Reward: 11.850819672131156\n",
      "Episode: 483, Total Reward: 7.350184501845053\n",
      "Episode: 484, Total Reward: 72.71558441558444\n",
      "Episode: 485, Total Reward: 7.0268882175227\n",
      "Episode: 486, Total Reward: 6.821146953405053\n",
      "Episode: 487, Total Reward: 6.505633802816941\n",
      "Episode: 488, Total Reward: 36.8041916167664\n",
      "Episode: 489, Total Reward: 6.082130584192479\n",
      "Episode: 490, Total Reward: 86.22359550561812\n",
      "Episode: 491, Total Reward: 9.034228187919503\n",
      "Episode: 492, Total Reward: 86.47355371900836\n",
      "Episode: 493, Total Reward: 10.176595744680872\n",
      "Episode: 494, Total Reward: 17.6769784172661\n",
      "Episode: 495, Total Reward: 30.711846689895417\n",
      "Episode: 496, Total Reward: 65.74107744107748\n",
      "Episode: 497, Total Reward: 22.122591362126183\n",
      "Episode: 498, Total Reward: 66.22500000000002\n",
      "Episode: 499, Total Reward: 15.390066225165489\n",
      "Episode: 500, Total Reward: 8.507843137254941\n"
     ]
    }
   ],
   "source": [
    "def process_state_image(state):\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = state.astype(float)\n",
    "    return state / 255.0\n",
    "\n",
    "action_space = [\n",
    "    (-1, 1, 0.5), (0, 1, 0.5), (1, 1, 0.5), # Action Space Structure\n",
    "    (-1, 1,   0), (0, 1,   0), (1, 1,   0), # (Steering Wheel, Gas, Break)\n",
    "    (-1, 0, 0.5), (0, 0, 0.5), (1, 0, 0.5), # Range -1~1       0~1   0~1\n",
    "    (-1, 0,   0), (0, 0,   0), (1, 0,   0)\n",
    "]\n",
    "state_shape = (96, 96, 1)\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "env = gym.make(\"CarRacing-v2\")\n",
    "agent = DQN(action_space, state_shape)\n",
    "agent.model.load_weights(\"models/trial_385o/dqn\")\n",
    "agent.epsilon = agent.epsilon_min\n",
    "\n",
    "num_episodes = 500\n",
    "for episode in range(385, num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = process_state_image(state)\n",
    "    total_reward = 0\n",
    "    negative_reward = 0\n",
    "    frame_counter = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        next_state = process_state_image(next_state)\n",
    "\n",
    "        total_reward += reward\n",
    "        frame_counter += 1\n",
    "\n",
    "        if frame_counter > 100 and reward < 0:\n",
    "            negative_reward += 1\n",
    "        else:\n",
    "            negative_reward = 0\n",
    "\n",
    "        if negative_reward > 10 or total_reward < 0:\n",
    "            done = True # End early\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        if frame_counter % 5 == 0:\n",
    "            agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    if episode > 0 and episode % 10 == 0:\n",
    "        # Save model every 10th episode\n",
    "        agent.model.save_weights(f\"models/trial_{episode}/dqn\")\n",
    "    \n",
    "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
    "    reward_history += [total_reward]\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save_weights(f\"models/trial_500/dqn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Agent erreicht nach 500 Episoden einen durchschnittlichen Reward von 38."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
